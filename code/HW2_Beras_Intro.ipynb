{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e32efb-34fd-47c4-8614-b75587650151",
   "metadata": {},
   "source": [
    "# CS1470/2470 HW2: Multi-Layered Neural Networks\n",
    "\n",
    "In this homework assignment, you will build a sequential model using differential modules.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37151f-a883-4c0b-a09a-b68f5ff1bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Beras, assignment, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8351-fb6b-4e23-b9cd-7d7a951f8cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pull In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369b71e0-092e-4ad1-9d63-ee4fcc952a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shapes: (60000, 784) (60000,)\n",
      "Testing  Shapes: (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# ![ ! -d '../../data' ] && bash cd ../.. && bash download.sh\n",
    "data_path = \"../data\"\n",
    "\n",
    "## Import MNIST train and test examples into train and testing data\n",
    "from preprocess import load_and_preprocess_data\n",
    "\n",
    "## Read in MNIST data,\n",
    "X0, Y0, X1, Y1 = load_and_preprocess_data()\n",
    "\n",
    "print(\"Training Shapes:\", X0.shape, Y0.shape)\n",
    "print(\"Testing  Shapes:\", X1.shape, Y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2bd75-32c9-43ff-b74d-9ce3a470e473",
   "metadata": {},
   "source": [
    "**> Expected Output** (double-click)\n",
    "<!-- \n",
    "```\n",
    "Training Shapes: (60000, 784) (60000,)\n",
    "Testing  Shapes: (10000, 784) (10000,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0c2a1-f97a-4233-9612-bfecf93dd0eb",
   "metadata": {},
   "source": [
    "## Starting Our Modular API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c25c8-5f1a-47c5-8ebd-28c8e6c85a20",
   "metadata": {},
   "source": [
    "### **The goal of this assignment is as follows:** \n",
    "- Extend our knowledge of deep learning by extending our single-layer model intuitions into multi-layer extensions. \n",
    "- Get familiarized with a simple modular API which reflects (but is a simplification of) the Keras analogue. \n",
    "    - Specifically, the `SequentialModel`\n",
    "- Implement some nice modular components and be able to construct a functional single-file neural network from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a945cbd-4903-4f13-9693-b864e871a736",
   "metadata": {},
   "source": [
    "### **Exploring a possible modular implementation: TensorFlow/Keras**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e732371d",
   "metadata": {},
   "source": [
    "We can check out what an established deep learning framework does to help us motivate our model. You'll learn more about this in the TensorFlow lab, but you can define a deep learning architecture using, among many other options, the `SequentialModel`:\n",
    "\n",
    "```python \n",
    "from tensorflow.keras.optimizers import Adam                  ## Special\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Softmax ## Differentiable\n",
    "from tensorflow.keras.losses import MeanSquaredError          ## Differentiable\n",
    "from tensorflow.keras.metrics import MeanSquaredError         ## Non-Differentiable (but Callable)\n",
    "from tensorflow.keras import Sequential                       ## Differentiable (surprisingly enough)\n",
    "\n",
    "## Simple Model Architecture\n",
    "tf.keras.Sequential([\n",
    "    Dense(32),    ## ? -> 32 units per entry\n",
    "    LeakyReLU(),  ## Bind logits to [min(l)*alpha, max(l)]\n",
    "    Dense(1),     ## 4 -> 1 units per entry\n",
    "    Softmax()     ## Bind logits to [0, 1] such that sum(logits) = 1\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer   = Adam(learning_rate=1),\n",
    "    loss        = MeanSquaredError(),\n",
    "    metrics     = [MeanSquaredError()]\n",
    ")\n",
    "\n",
    "model.fit(X0, Y0, batch_size = 20, epochs = 10)\n",
    "model.evaluate(X0, Y0, batch_size = 100)\n",
    "```\n",
    "\n",
    "This shows the main Keras value proposition: Modular components which you can move around and customize to your heart's content. This allows for rapid prototyping and small code bases, which allows almost anybody to get started with deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaf3e1",
   "metadata": {},
   "source": [
    "Keras itself is made up of building blocks from (and is a well-defined part of) Tensorflow. \n",
    "- **Keras:** Gives you high-level building blocks to keep track of internal components and train models easily.\n",
    "- **Tensorflow**: Gives you the lower-level building blocks to easily differentiate and perform math operations. \n",
    "\n",
    "You'll find out more about this in the lab, but you can get a feel for how Tensorflow can be used by digging into the more low-level implementation of the Sequential Model *(which still uses Keras... I mean, we're not barbarians, right?)*: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eda366",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class SequentialModel(tf.keras.Model):\n",
    "    '''\n",
    "    Note, this is a simplification, but it's close enough...\n",
    "    It's also not exactly what you'll be doing (but really close)\n",
    "    Note that a lot of things are missing: That's because they're handled by tf.keras.Model\n",
    "    '''\n",
    "    def __init__(self, layers):\n",
    "        ## Phase at which you should specify the variables needed to do passes\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        ## Phase at which you specify forward propagation\n",
    "        x = tf.identity(inputs)   ## Copy input to de-reference it\n",
    "        for layer in layers: \n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, data):\n",
    "        ## Optional Train_Step Specification\n",
    "        ## This thing gets called for every batch\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:  ## While in scope of gradient tape: \n",
    "            ## Record what happens between all tf.Variable operations and \n",
    "            ## keep track of the partial gradients that get generated.\n",
    "            logits  = self.call(x)          ## This is differentiable (i.e. implemented w/ tf.Variables)\n",
    "            loss    = self.loss(y, logits)  ## This is also differentiable\n",
    "        \n",
    "        ## Figure out dL/dw for every trainable weights w based on \n",
    "        ## which operations were performed between them.\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea829e5e",
   "metadata": {},
   "source": [
    "So... make it! ... kinda...\n",
    "\n",
    "...yeah, that's a tall order. A few things to not about this:\n",
    "- Tensorflow uses tf.Variables to do auto-differentiation from the operator level onward, which includes support for basic processes like addition and quotients and a bunch of other things. That's way too much detail for us to implement! \n",
    "- Real models oftentimes have variables and representations that diverge paths before reconvening into loss evaluations. In order to handle this, you'd need to implement a graph structure. This would just distract us from the main focus.\n",
    "\n",
    "The sequential model allows us to simplify both of these away: \n",
    "- Since we only use large modules (i.e. Dense, ReLU, Sigmoid, MeanSquaredError, etc.), we can implement differentiation on that level: This will prove to be surprisingly tractible – but not exactly trivial, hence the assignment.\n",
    "- Since the sequential model by default assumes a one-module-at-a-time pass-through, we don't have to worry too much about implicitly supporting primitive operations that cause sophisticated branching. \n",
    "    - Most components will only require one pathway in back-propagation: towards the partial with respect to its inputs. \n",
    "    - The dense layer will require the most amount of branching among our (1470) components, since it requires partials with respect to inputs, weights, and biases. We can handle that, right...?\n",
    "    \n",
    "Also note that the point of this exercise is not to implement a real library, nor is it to implement TF/Keras faithfully. As such, there is little consideration for realistic optimization (aside from vectorization) or scale beyond the assignment.\n",
    "\n",
    "A more realistic (though still very pedagogical) implementation named [Brunoflow](https://github.com/Brown-Deep-Learning/brunoflow) is available from our very own Daniel Ritchie! It is a really nice library, but unfortunately is a bit too much for students to implement (and is also publicly available). Feel free to look at it whenever you want, as the two versions are very different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db05df1-6889-4280-ab95-48109596feb4",
   "metadata": {},
   "source": [
    "### Testing out our first Callable module: **OneHotEncoder**\n",
    "\n",
    "To get you all warmed up to the idea of the modules, we have already provided you with an implementation of the `OneHotEncoder`.\n",
    "\n",
    "- **`OneHotEncoder`**\n",
    "    - **Pre-processing step:** Takes label dataset and converts each label $\\ell \\in L$ such that $\\text{ohe}(\\ell) = \\mathbb{1}_{i=0}^{\\|L\\|}(i = \\ell)$\n",
    "    - **Callable:** Does not need to be differentiated, since not in optimization loop.\n",
    "    - **Stateful:** Has to keep track of some states. For example, the set of unique elements and their mappings to the one-hot vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42bfb7f9-b31b-462a-96f9-ee9981949a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%aimport` not found.\n"
     ]
    }
   ],
   "source": [
    "%aimport Beras\n",
    "\n",
    "from abc import ABC, abstractmethod  ## For abstract method support\n",
    "import numpy as np\n",
    "\n",
    "class Callable:\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        '''Lets `self()` and `self.call()` be the same'''\n",
    "        return self.call(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def call(self, *args, **kwargs):\n",
    "        \"\"\"Propagates input through the network. Stores inputs and outputs as instance variables\"\"\"\n",
    "        pass\n",
    "\n",
    "class OneHotEncoder(Callable):\n",
    "    '''\n",
    "    One-Hot Encodes labels. First takes in a fit-set to figure out what elements it \n",
    "    needs to consider, and then one-hot encodes subsequent input datasets in the \n",
    "    forward pass (call). \n",
    "    \n",
    "    SIMPLIFICATIONS: \n",
    "     - Implementation assumes that entries are individual elements.\n",
    "     - keras does not have OneHotEncoder; has LabelEncoder, CategoricalEncoder, and to_categorical()\n",
    "    '''  \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Fits the one-hot encoder to a candidate dataset. Said dataset should contain \n",
    "        all encounterable elements.\n",
    "        '''\n",
    "        self.uniq = np.unique(data)\n",
    "        self.vecs = np.eye(len(self.uniq))\n",
    "        self.uniq2oh = {e : self.vecs[i] for i, e in enumerate(self.uniq)} \n",
    "\n",
    "    def call(self, data):\n",
    "        if not hasattr(self, 'uniq2oh'): self.fit(data)\n",
    "        return np.array([self.uniq2oh[x] for x in data])\n",
    "\n",
    "    def inverse(self, data):\n",
    "        ## Special custom method useful for OHE. Not generally expected in other instances\n",
    "        assert hasattr(self, 'uniq'), 'call() or fit() must be called before attempting to invert'\n",
    "        return np.array([self.uniq[x == 1][0] for x in data])\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.concatenate([Y0, Y1], axis=-1))\n",
    "\n",
    "print(\"Getting label sample:\")\n",
    "print(Y0[:10])\n",
    "print(\"Testing Call:\")\n",
    "print(''.join([f'  {i}' for i in range(10)]), 'is hot')\n",
    "print(ohe(Y0[:10]))\n",
    "print(\"Testing Inverse:\")\n",
    "print(ohe.inverse(ohe(Y0[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791f711-6313-4d66-91d1-2807c68fb308",
   "metadata": {},
   "source": [
    "### Testing out our first Diffable module: **Categorical Cross-Entropy**\n",
    "\n",
    "An extension of Callable – a module that can be called – is the Diffable – a module that can be Diff(erentiat)ed. The details are quite... specific. We (or at least Vadim) originally wanted you all to do this, but then we realized that this would take an extremely long amount of time to implement and debug even if you already knew all of the concepts. So instead, we've provided the source code for this whole part (in fact, almost all of `Beras/core.py`) in the assignment stencil. Sometimes it's better to have you struggle to reason about a well-debugged implementation than to actually have you implement it yourself... or at least that's the mindset we're going with. Hope you appreciate it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d4cd0",
   "metadata": {},
   "source": [
    "```python\n",
    "class Diffable(ABC, Callable):\n",
    "\n",
    "    '''Is the operation being used inside a gradient tape scope?'''\n",
    "    gradient_tape = None    ## All-instance-shared variable\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        '''\n",
    "        If there is a gradient tape scope in effect, perform AND RECORD the operation.\n",
    "        Otherwise... just perform the operation and don't let the gradient tape know. \n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def input_gradients(self):\n",
    "        \"\"\"Returns gradient for input (this part gets specified for all diffables)\"\"\"\n",
    "\n",
    "    def weight_gradients(self):\n",
    "        \"\"\"Returns gradient for input (this part gets specified for SOME diffables)\"\"\"\n",
    "\n",
    "    def compose_to_input(self, J):\n",
    "        \"\"\"\n",
    "        Compose the inputted cumulative jacobian with the input jacobian for the layer.\n",
    "        Implemented with batch-level vectorization.\n",
    "\n",
    "        Requires `input_gradients` to provide either batched or overall jacobian.\n",
    "        Assumes input/cumulative jacobians are matrix multiplied\n",
    "        \"\"\"\n",
    "\n",
    "    def compose_to_weight(self, J):\n",
    "        \"\"\"\n",
    "        Compose the inputted cumulative jacobian with the weight jacobian for the layer.\n",
    "        Implemented with batch-level vectorization.\n",
    "\n",
    "        Requires `weight_gradients` to provide either batched or overall jacobian.\n",
    "        Assumes weight/cumulative jacobians are element-wise multiplied (w/ broadcasting)\n",
    "        and the resulting per-batch statistics are averaged together for avg per-param gradient.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6b639",
   "metadata": {},
   "source": [
    "#### **Simple Diffable Example:** Sin Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport Beras\n",
    "\n",
    "from Beras.core import Diffable\n",
    "import numpy as np\n",
    "\n",
    "class Sin(Diffable):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return np.sin(inputs)\n",
    "\n",
    "    def input_gradients(self):\n",
    "        return [np.cos(self.inputs)]\n",
    "        \n",
    "    def compose_to_input(self, J):\n",
    "        ## One of the few times you may want to do this.\n",
    "        ## We'll leave it up to you to figure out when.\n",
    "        return self.input_gradients()[0] * J\n",
    "\n",
    "act_fn = Sin()\n",
    "## 2 batches with 3 entries and 4 elements per entry\n",
    "sample = np.arange(-12, 12).reshape(2, 3, 4)       \n",
    "out = act_fn(sample)\n",
    "\n",
    "print(\"Activation Input:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nActivation Output:\")\n",
    "print(out)\n",
    "\n",
    "print(\"\\nInput Gradients:\")\n",
    "print(act_fn.input_gradients())\n",
    "\n",
    "print(\"\\nCompose To Input:\")\n",
    "print(act_fn.compose_to_input(out))\n",
    "\n",
    "print(\"\\nSanity Check:\")\n",
    "comp1 = act_fn.compose_to_input(act_fn.input_gradients())[0]\n",
    "comp2 = act_fn.input_gradients()[0] * act_fn.input_gradients()[0]\n",
    "if np.allclose(comp1, comp2):\n",
    "    print(\"The world still makes sense\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b426ddaf",
   "metadata": {},
   "source": [
    "#### **Advanced Diffable Example:** MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a0ba1-a1e5-45e5-9a1a-44cb94a29594",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport Beras\n",
    "\n",
    "from Beras.core import Diffable\n",
    "import numpy as np\n",
    "\n",
    "class MeanSquaredError(Diffable):\n",
    "\n",
    "    def __init__(self, *args, from_logits=True, **kwargs):\n",
    "        ## We're redirecting all of our arguments but also supporting an extra one!\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_pred, y_true):\n",
    "        \"\"\"Mean squared error forward pass!\"\"\"\n",
    "        if self.from_logits:\n",
    "            mse_total = np.mean(np.power(y_pred - y_true, 2), axis=-1)\n",
    "        else: \n",
    "            mse_total = np.mean(np.power(y_pred * y_true - y_true, 2), axis=-1)\n",
    "        return np.mean(mse_total, axis=0)\n",
    "\n",
    "    def input_gradients(self):\n",
    "        \"\"\"Mean squared error input gradient!\"\"\"\n",
    "        y_pred, y_true = self.inputs\n",
    "        if self.from_logits:\n",
    "            ## Using all entries. Sub-par if using softmax\n",
    "            grad = 2 * (y_pred - y_true) / np.prod(y_pred.shape)\n",
    "        else:\n",
    "            ## Only using target class. Better for softmax\n",
    "            grad = 2 * ((y_pred * y_true) - y_true) / y_pred.shape[0]\n",
    "        return [grad, -grad]\n",
    "    \n",
    "loss_fn = MeanSquaredError()\n",
    "## 3 batches with 3 elements per batch (losses, right?)\n",
    "ones = np.ones((3, 3)) \n",
    "zeros = np.zeros_like(ones)\n",
    "\n",
    "print(\"Awful performance\")\n",
    "print(loss_fn(ones, zeros))\n",
    "print(loss_fn(zeros, ones))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nPerfect performance\")\n",
    "print(loss_fn(ones, ones))\n",
    "print(loss_fn(zeros, zeros))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nNot great performance\")\n",
    "print(loss_fn(ones * 0.5, ones))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nWeight Gradients:\")\n",
    "print(loss_fn.weight_gradients())\n",
    "\n",
    "print(\"\\nCompose To Input:\")\n",
    "print(loss_fn.compose_to_input())\n",
    "\n",
    "print(\"\\nSanity Check:\")\n",
    "comp1 = loss_fn.compose_to_input(loss_fn.input_gradients())[0]\n",
    "comp2 = loss_fn.input_gradients()[0] @ loss_fn.input_gradients()[0]\n",
    "if np.allclose(comp1, comp2):\n",
    "    print(\"The world still makes sense\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "759be6693a164ddeab1e231298c2a01a8302a7c7dfd4e560844dbce42a896f34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
